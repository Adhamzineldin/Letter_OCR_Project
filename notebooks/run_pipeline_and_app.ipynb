{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMNIST Letter OCR – Run Pipelines and App\n",
    "\n",
    "This notebook lets you:\n",
    "\n",
    "- Run several `OCRPipeline` configurations (baseline, augmentation, PCA).\n",
    "- Save artifacts for the GUI.\n",
    "- Plot a confusion matrix for the best model.\n",
    "- Optionally launch the Streamlit app.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from src.ocr_project.pipeline import OCRPipeline\n",
    "from src.ocr_project.features.pca_features import PCAFeatures\n",
    "from src.ocr_project import config\n",
    "\n",
    "\n",
    "def run_pipeline_config(name: str, **kwargs):\n",
    "    print(f\"\\n=== Running pipeline: {name} ===\")\n",
    "    pipeline = OCRPipeline(**kwargs)\n",
    "    results = pipeline.run()\n",
    "    for model_name, result in results.items():\n",
    "        print(f\"  {model_name}: accuracy = {result.accuracy:.4f}\")\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Baseline: small subset, no augmentation, raw pixels\n",
    "baseline_results = run_pipeline_config(\n",
    "    \"baseline_small\",\n",
    "    train_limit=10000,\n",
    "    test_limit=2000,\n",
    "    use_augmentation=False,\n",
    "    feature_extractor=None,\n",
    ")\n",
    "\n",
    "# 2) Augmented data, same size, raw pixels\n",
    "aug_results = run_pipeline_config(\n",
    "    \"augmented_small\",\n",
    "    train_limit=10000,\n",
    "    test_limit=2000,\n",
    "    use_augmentation=True,\n",
    "    feature_extractor=None,\n",
    ")\n",
    "\n",
    "# 3) Augmented + PCA features on a larger subset\n",
    "pca_results = run_pipeline_config(\n",
    "    \"augmented_pca\",\n",
    "    train_limit=20000,\n",
    "    test_limit=5000,\n",
    "    use_augmentation=True,\n",
    "    feature_extractor=PCAFeatures(n_components=50),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix for the Random Forest model\n",
    "\n",
    "from src.ocr_project.io.dataset_loader import EmnistLoader\n",
    "from src.ocr_project.io.persistence import load_model_artifact\n",
    "from src.ocr_project.preprocess.transformer import Transformer\n",
    "from src.ocr_project.evaluation.visualizer import Visualizer\n",
    "\n",
    "# Load test data (same limits as in the PCA run)\n",
    "loader = EmnistLoader(config.EMNIST_LETTERS_TRAIN, config.EMNIST_LETTERS_TEST)\n",
    "test_split = loader.load_test(limit=5000)\n",
    "X_test = Transformer.normalize(Transformer.flatten(test_split.images))\n",
    "y_test = test_split.labels\n",
    "\n",
    "# Load the Random Forest artifact produced by the last pipeline run\n",
    "rf_artifact = load_model_artifact(Path(\"artifacts\") / \"random_forest.pkl\")\n",
    "rf_model = rf_artifact[\"model\"]\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "Visualizer.plot_confusion_matrix(y_test, y_pred, normalize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: launch the Streamlit app from the notebook (may block the kernel)\n",
    "# You can also run this from a terminal instead:\n",
    "#   streamlit run app.py\n",
    "\n",
    "!streamlit run app.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMNIST Letter OCR – Pipeline and App\n",
    "\n",
    "This notebook lets you:\n",
    "\n",
    "- Train the Decision Tree and Random Forest models via the `OCRPipeline` class.\n",
    "- Save model artifacts for the Streamlit GUI.\n",
    "- (Optionally) launch the Streamlit app from within the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T11:12:08.405651Z",
     "start_time": "2025-12-03T11:10:55.459965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree: accuracy = 0.5574\n",
      "random_forest: accuracy = 0.8354\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from src.ocr_project.pipeline import OCRPipeline, run_default_pipeline\n",
    "from src.ocr_project import config\n",
    "\n",
    "# Option 1: use the helper that relies on default config\n",
    "results = run_default_pipeline()\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"{name}: accuracy = {result.accuracy:.4f}\")\n",
    "\n",
    "# Option 2: create and run a custom pipeline (example)\n",
    "# pipeline = OCRPipeline(\n",
    "#     train_csv=config.EMNIST_LETTERS_TRAIN,\n",
    "#     test_csv=config.EMNIST_LETTERS_TEST,\n",
    "#     artifacts_dir=Path(\"artifacts\"),\n",
    "# )\n",
    "# results = pipeline.run()\n",
    "# results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: launch the Streamlit app from the notebook (may block the kernel)\n",
    "# You can also run this from a terminal instead:\n",
    "#   streamlit run app.py\n",
    "\n",
    "!streamlit run app.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
